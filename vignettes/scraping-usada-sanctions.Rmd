---
title: "Scraping USADA sanctions"
output: 
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 4
    df_print: kable
vignette: >
  %\VignetteIndexEntry{Scraping USADA sanctions}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
options(
  pillar.print_max = 25, 
  dplyr.print_max = 25,
  knitr.kable.NA = '',
  pillar.print_min = 25)
library(polite)
library(dopingdata)
library(dplyr)
library(tidyr)
library(stringr)
library(forcats)
library(lubridate)
library(readr)
library(readxl)
library(janitor)
library(anytime)
library(robotstxt)
options(robotstxt_warn = FALSE)
```

Packages:

```{r pkg, eval=FALSE, message=FALSE, warning=FALSE}
library(dopingdata)
library(dplyr)
library(tidyr)
library(stringr)
library(forcats)
library(lubridate)
library(readr)
library(readxl)
library(janitor)
library(anytime)
library(robotstxt)
options(robotstxt_warn = FALSE)
```

## USADA sanction data

The data I'll be downloading comes from [USADA sanctions table](https://www.usada.org/testing/results/sanctions/).

## Use your manners 

Because this package is built on top of the efforts of the fine people who collected, organized, and shared their data, we're going to use the [`polite` package](https://dmi3kno.github.io/polite/) for harvesting the HTML tables. 

To install this package, run the code below: 

```{r inst-polite, eval=FALSE}
remotes::install_github("dmi3kno/polite")
library(polite)
```

`polite` has many options for ethically scraping data (check out the [package website](https://dmi3kno.github.io/polite/reference/index.html) for more information), but I've chosen to follow the handy [polite template](https://dmi3kno.github.io/polite/#polite-template):

```{r use_manners, eval=FALSE}
polite::use_manners()
```


### Check `robots.txt`

I'll check the `robots.txt` file before scraping the website:

```{r rt}
# retrieval
rtxt <- robotstxt::robotstxt(domain = "https://www.usada.org/")

# printing
rtxt$check(
  # check permissions 
  paths = c("testing/", 
            "testing/results/", 
            "testing/results/sanctions/"),
  # bots
  bot   = "*"
)
```

All three paths are `TRUE`, but I will also check the domain with `robotstxt::get_robotstxt()`:

```{r get_robotstxt}
rt <- robotstxt::get_robotstxt(
  domain = "https://www.usada.org/testing/results/sanctions/")
# printing
cat(rt[1])
```

I can see the `Allow: /` configuration [gives us access to download](https://kinsta.com/blog/wordpress-robots-txt/#how-to-use-robotstxt-allow-all-to-give-robots-full-access-to-your-site) the data. 


### Scraping with `polite` and `rvest`

Below are the steps used to scrape the sanctions table:

```{r scrape, include=FALSE, eval=FALSE}
usada_url = "https://www.usada.org/testing/results/sanctions/"
# extract html
usada_extraction <- rvest::read_html(usada_url)
# nodes
usada_nodes <- rvest::html_nodes(usada_extraction, "table")
# raw data
usada_sanctions_raw <- rvest::html_table(usada_nodes[[1]]) |> 
  tibble::as_tibble()
```

```{r polite-scraping, eval=FALSE}
usada_url = "https://www.usada.org/testing/results/sanctions/"
usada_nodes <- polite::bow(usada_url) |> 
  polite::scrape() |> 
  rvest::html_nodes("table") 
usada_sanctions_raw <- rvest::html_table(usada_nodes[[1]])
```

```{r hidden-usada_sanctions_raw-import, message=FALSE, warning=FALSE, echo=FALSE}
usada_sanctions_raw <- readr::read_csv(
  '../inst/extdata/raw/2023-01-22_usada_sanctions_raw.csv')
```

What does the raw data look like? 

```{r glimpse-usada_sanctions_raw}
dplyr::glimpse(usada_sanctions_raw)
```

You can also use the `scrape_usada_sanctions()` function. 

## Process text 

The raw HTML table needs some reformatting/restructuring before I can build any visualizations or models. I'll start by standardizing the names and making all the text lowercase. 

```{r clean_names-str_to_lower}
usada_sanctions_lc <- janitor::clean_names(usada_sanctions_raw) |> 
                     purrr::map_df(.f = str_to_lower)
glimpse(usada_sanctions_lc)
```

This takes care of the column names and case, but what about all the invisible characters? (wait, what is a [invisible character???](https://invisible-characters.com/)).

For cleaning text, we can use the `process_text()` function, which adds a few steps: 

1. replaces carriage returns (`\\r`) and newlines (`\\n`) with white space  
2. converts **all** variables to text (`chr`)  

```{r usada_sanctions_pro}
usada_sanctions_pro <- process_text(usada_sanctions_raw)
dplyr::glimpse(usada_sanctions_pro)
```

If I compare with `waldo::compare()`, I can see the differences: 

```{r waldo-compare}
waldo::compare(usada_sanctions_lc, usada_sanctions_pro)
```

## Exporting the raw data

I'll export the raw data into the `inst/extdata/raw/` folder. I've written a custom function for date-stamping the raw data before exporting: 

```{r export_extdata-raw}
export_extdata(x = usada_sanctions_raw, 
  inst_path = "../inst/", raw = TRUE)
```

Verify: 

```{r verify}
fs::dir_tree("../inst/extdata/raw/")
```

## Export processed data 

I want to export the processed data into the `inst/extdata/` folder, which I can create using `export_extdata(raw = FALSE)`

```{r export_extdata-processed}
export_extdata(x = usada_sanctions_pro, 
  inst_path = "../inst/", raw = FALSE)
```

Verify: 

```{r processed-verify}
fs::dir_tree(path = '../inst/extdata/', regexp = "sanctions_pro")
```
